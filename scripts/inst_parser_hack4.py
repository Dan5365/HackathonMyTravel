import os
import csv
import json
import time
import random
import math
from datetime import datetime, timedelta, timezone
from pydantic import ValidationError

from instagrapi import Client
from instagrapi.exceptions import UserNotFound
from instagrapi.types import Media, User, Location
from instagrapi.extractors import extract_media_v1

# --- –ù–ê–°–¢–†–û–ô–ö–ò –°–ö–†–ò–ü–¢–ê ---
USERNAME = "USER"
PASSWORD = "Password"
SESSION_FILE = "instagram_sessions/my_instagram_session.json"
INPUT_CSV_FILE = "found_users.csv"
OUTPUT_CSV_FILE = "instagram_data_summary.csv"
OUTPUT_JSON_REPORT_FILE = "instagram_data_full_report.json"
POSTS_TO_FETCH = 10
CAPTION_TRUNCATE_LIMIT = 300


# --- –ù–ê–°–¢–†–û–ô–ö–ò –ü–û–ò–°–ö–ê ---
SEARCH_KEYWORDS = ["–≥–ª—ç–º–ø–∏–Ω–≥", "–≥–ª—ç–º–ø–∏–Ω–≥ –∞—Å—Ç–∞–Ω–∞"]
SEARCH_CITY = "–ê—Å—Ç–∞–Ω–∞" 
SEARCH_LIMIT_PER_QUERY = 30 
FOUND_USERS_CSV_FILE = "found_users.csv"



def human_delay(min_seconds=5, max_seconds=12):
    delay = random.uniform(min_seconds, max_seconds)
    print(f"‚è≥ –ü–∞—É–∑–∞ –Ω–∞ {delay:.2f} —Å–µ–∫—É–Ω–¥...")
    time.sleep(delay)


def extract_hashtags(text: str) -> list:
    if not text: return []
    return [tag.strip("#") for tag in text.split() if tag.startswith("#")]


def calculate_metrics(posts: list, followers_count: int) -> dict:
    metrics = {"avg_likes": 0, "avg_comments": 0, "engagement_rate_percent": 0.0, "posting_frequency_days": None, "activity_score": 0.0}
    if not posts: return metrics
    
    num_posts = len(posts)
    metrics["avg_likes"] = sum(p.like_count for p in posts) / num_posts
    metrics["avg_comments"] = sum(p.comment_count for p in posts) / num_posts

    if followers_count > 0:
        metrics["engagement_rate_percent"] = ((metrics["avg_likes"] + metrics["avg_comments"]) / followers_count) * 100
    
    if num_posts > 1:
        posts_sorted = sorted(posts, key=lambda p: p.taken_at, reverse=True)
        time_diffs = [(posts_sorted[i].taken_at - posts_sorted[i+1].taken_at).total_seconds() for i in range(num_posts - 1)]
        metrics["posting_frequency_days"] = (sum(time_diffs) / len(time_diffs)) / 86400
    
    now = datetime.now(timezone.utc)
    posts_last_30_days = sum(1 for p in posts if p.taken_at > now - timedelta(days=30))
    score_posts = min(posts_last_30_days / 10, 1) * 5
    score_engagement = min(metrics["engagement_rate_percent"] / 5, 1) * 5
    metrics["activity_score"] = round(score_posts + score_engagement, 1)
    
    return metrics


def calculate_lead_analysis(profile_data: dict, calculated_metrics: dict) -> dict:
    followers = profile_data.get('followers_count', 0)
    popularity = round(min(math.log10(followers or 1) * 1.6, 10), 1)

    completeness_score = 0
    if profile_data.get('bio'): completeness_score += 3
    if profile_data.get('website'): completeness_score += 3
    if profile_data.get('is_business'): completeness_score += 2
    if profile_data.get('business_category'): completeness_score += 2
    data_completeness = float(completeness_score)
    
    er = calculated_metrics.get('engagement_rate_percent', 0.0)
    potential_score = 0
    potential_score += min(er / 2.5, 1) * 4
    if profile_data.get('is_business'): potential_score += 2
    if profile_data.get('website'): potential_score += 2
    potential_score += popularity / 10 * 2
    commercial_potential = round(potential_score, 1)

    network_activity = calculated_metrics.get('activity_score', 0.0)
    final_score = round((network_activity * 0.4) + (commercial_potential * 0.4) + (popularity * 0.1) + (data_completeness * 0.1), 1)
    
    if final_score >= 8.0: priority = "hot"
    elif final_score >= 5.0: priority = "medium"
    else: priority = "low"

    return {
        "network_activity": network_activity, "data_completeness": data_completeness,
        "popularity": popularity, "filling_potential": 10.0 - data_completeness,
        "target_audience_fit": None, "commercial_potential": commercial_potential,
        "final_score": final_score, "priority": priority
    }


def search_and_prepare_users(cl: Client, keywords: list, city: str, limit: int):
    if not keywords and not city:
        print("‚ÑπÔ∏è –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ –≥–æ—Ä–æ–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–µ —É–∫–∞–∑–∞–Ω—ã. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–∞–ø –ø–æ–∏—Å–∫–∞.")
        return

    print("\n--- üîç –ù–ê–ß–ê–õ–û –ü–û–ò–°–ö–ê –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ô ---")
    found_users_data = {}

    if city:
        print(f"üìç –ò—â–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ –≥–æ—Ä–æ–¥—É: {city}")
        try:
            
            locations = cl.location_search(city)
            if not locations:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ª–æ–∫–∞—Ü–∏—é –¥–ª—è –≥–æ—Ä–æ–¥–∞ '{city}'.")
            else:
                location = locations[0]
                print(f"  > –ù–∞–π–¥–µ–Ω–∞ –ª–æ–∫–∞—Ü–∏—è: {location.name} (ID: {location.pk})")
                medias = cl.location_medias_top(location.pk, amount=limit)
                print(f"  > –ù–∞–π–¥–µ–Ω–æ {len(medias)} –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ –≤ —ç—Ç–æ–π –ª–æ–∫–∞—Ü–∏–∏.")
                for media in medias:
                    if not hasattr(media, 'user'): continue
                    user = media.user
                    if user.username not in found_users_data:
                         found_users_data[user.username] = {
                            "username": user.username, "full_name": user.full_name, "followers": "", "bio": "",
                            "city": city, "link": f"https://www.instagram.com/{user.username}/"
                        }
                human_delay(3, 7)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ –≥–æ—Ä–æ–¥—É '{city}': {e}")

    if keywords:
        for keyword in keywords:
            print(f"üîë –ò—â–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É: '{keyword}'")
            try:
                results = cl.search_users(keyword)[:limit]
                print(f"  > –ù–∞–π–¥–µ–Ω–æ {len(results)} –∞–∫–∫–∞—É–Ω—Ç–æ–≤.")
                for user in results:
                    if user.username not in found_users_data:
                        
                        found_users_data[user.username] = {
                            "username": user.username, "full_name": user.full_name,
                            "followers": "", 
                            "bio": "",      
                            "city": "", "link": f"https://www.instagram.com/{user.username}/"
                        }
                human_delay(3, 7)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É '{keyword}': {e}")
    
    if not found_users_data:
        print("ü§∑‚Äç‚ôÇÔ∏è –ù–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        print("--- üèÅ –ü–û–ò–°–ö –ó–ê–í–ï–†–®–ï–ù ---\n")
        return

    print(f"\n‚úÖ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(found_users_data)}")
    found_users_list = list(found_users_data.values())
    
    try:
        with open(FOUND_USERS_CSV_FILE, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=["username", "full_name", "followers", "bio", "city", "link"])
            writer.writeheader()
            writer.writerows(found_users_list)
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {FOUND_USERS_CSV_FILE}")
    except Exception as e:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ {FOUND_USERS_CSV_FILE}: {e}")

    try:
        existing_usernames = set()
        if os.path.exists(INPUT_CSV_FILE):
            with open(INPUT_CSV_FILE, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if 'username' in row:
                        existing_usernames.add(row['username'].strip())
        
        new_users_to_add = [ user for user in found_users_data.keys() if user not in existing_usernames ]
        if new_users_to_add:
            print(f"‚ûï –î–æ–±–∞–≤–ª—è–µ–º {len(new_users_to_add)} –Ω–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ {INPUT_CSV_FILE} –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.")
            with open(INPUT_CSV_FILE, 'a', newline='', encoding='utf-8') as f:
                is_file_empty = os.stat(INPUT_CSV_FILE).st_size == 0
                writer = csv.DictWriter(f, fieldnames=['object_id', 'username'])
                if is_file_empty: writer.writeheader()
                for username in new_users_to_add:
                    writer.writerow({'object_id': '', 'username': username})
        else:
            print("üëç –í—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —É–∂–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Ñ–∞–π–ª–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
    except Exception as e:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å —Ñ–∞–π–ª {INPUT_CSV_FILE}: {e}")

    print("--- üèÅ –ü–û–ò–°–ö –ó–ê–í–ï–†–®–ï–ù ---\n")


def get_user_posts_robust(cl: Client, user_id: str, amount: int) -> list[Media]:
    posts = []
    try:
        
        response = cl.private_request(f'feed/user/{user_id}/', params={'count': amount})
        
      
        raw_medias = response.get('items', [])
        if not isinstance(raw_medias, list):
            print(f"  - ‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç API. –ö–ª—é—á 'items' –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ø–∏—Å–∫–æ–º.")
            return []

        for raw_media in raw_medias:
            try:
                
                posts.append(extract_media_v1(raw_media))
            except ValidationError as e:
                media_pk = raw_media.get('pk', 'N/A')
               
                error_msg = e.errors()[0]['msg'] if e.errors() else "Unknown validation error"
                print(f"  - ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω –ø–æ—Å—Ç (ID: {media_pk}) –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {error_msg}")
                continue 
    except Exception as e:
         print(f"  - ‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ø–æ—Å—Ç–æ–≤: {e}")
    return posts


def main():
    cl = Client()
  
    session_dir = os.path.dirname(SESSION_FILE)
    if session_dir: os.makedirs(session_dir, exist_ok=True)

    if os.path.exists(SESSION_FILE):
        try:
            cl.load_settings(SESSION_FILE); cl.login(USERNAME, PASSWORD)
            print("‚úÖ –í—Ö–æ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π —Å–µ—Å—Å–∏–∏.")
        except Exception:
            print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ–π—Ç–∏ —Å —Å–µ—Å—Å–∏–µ–π. –ü–æ–ø—ã—Ç–∫–∞ –≤—Ö–æ–¥–∞ –ø–æ –ª–æ–≥–∏–Ω—É –∏ –ø–∞—Ä–æ–ª—é.")
            cl.login(USERNAME, PASSWORD); cl.dump_settings(SESSION_FILE)
            print("‚úÖ –í—ã–ø–æ–ª–Ω–µ–Ω —Å–≤–µ–∂–∏–π –≤—Ö–æ–¥, —Å–µ—Å—Å–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
    else:
        print("‚ÑπÔ∏è –§–∞–π–ª —Å–µ—Å—Å–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω. –í—Ö–æ–¥ –ø–æ –ª–æ–≥–∏–Ω—É –∏ –ø–∞—Ä–æ–ª—é.")
        cl.login(USERNAME, PASSWORD); cl.dump_settings(SESSION_FILE)
        print("‚úÖ –í—Ö–æ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω, —Å–µ—Å—Å–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")

    search_and_prepare_users(cl, SEARCH_KEYWORDS, SEARCH_CITY, SEARCH_LIMIT_PER_QUERY)

    try:
        with open(INPUT_CSV_FILE, mode='r', encoding='utf-8') as f:
            non_empty_lines = (line for line in f if line.strip())
            users_to_process = list(csv.DictReader(non_empty_lines))
    except FileNotFoundError:
        print(f"‚ùå –û–®–ò–ë–ö–ê: –í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª '{INPUT_CSV_FILE}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –ê–Ω–∞–ª–∏–∑ –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω.")
        return
    
    if not users_to_process:
        print(f"ü§∑‚Äç‚ôÇÔ∏è –§–∞–π–ª '{INPUT_CSV_FILE}' –ø—É—Å—Ç. –ù–µ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å.")
        return

    print(f"\n--- üöÄ –ù–ê–ß–ê–õ–û –ì–õ–£–ë–û–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê ---")
    print(f"üìà –ù–∞–π–¥–µ–Ω–æ {len(users_to_process)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")

    csv_fieldnames = [
        'object_id', 'username', 'status', 'la_priority', 'la_final_score',
        'display_name', 'followers_count', 'following_count', 'posts_count', 
        'bio', 'website', 'is_business', 'business_category',
        'avg_likes', 'avg_comments', 'engagement_rate_percent', 'posting_frequency_days', 'activity_score',
        'la_network_activity', 'la_commercial_potential', 'la_popularity', 'la_data_completeness', 'la_filling_potential', 'la_target_audience_fit',
        'instagram_url'
    ]
    
    all_users_full_data = []
    
    with open(OUTPUT_CSV_FILE, 'w', newline='', encoding='utf-8') as csv_file:
        writer = csv.DictWriter(csv_file, fieldnames=csv_fieldnames)
        writer.writeheader()

        for i, user_row in enumerate(users_to_process):
            username = user_row.get('username', '').strip()
            if not username: continue
            
            print(f"\n--- –ê–Ω–∞–ª–∏–∑ {i+1}/{len(users_to_process)}: {username} ---")
            
            result_row = {'object_id': user_row.get('object_id', ''), 'username': username, 'instagram_url': f"https://www.instagram.com/{username}/", 'status': 'ERROR'}

            try:
                user_info: User = cl.user_info_by_username_v1(username)
                
                profile_data = {
                    "username": user_info.username, "display_name": user_info.full_name,
                    "followers_count": user_info.follower_count, "following_count": user_info.following_count,
                    "posts_count": user_info.media_count, "bio": user_info.biography,
                    "website": str(user_info.external_url) if user_info.external_url else '',
                    "is_business": user_info.is_business, "business_category": getattr(user_info, 'category_name', '')
                }

                if user_info.is_private:
                    result_row['status'] = 'PRIVATE'; print("üîí –ü—Ä–æ—Ñ–∏–ª—å –ø—Ä–∏–≤–∞—Ç–Ω—ã–π.")
                    result_row.update(profile_data)
                    writer.writerow(result_row); human_delay(2, 5); continue
                
                result_row.update({'status': 'OK', **profile_data})
                
                print(f"üìÑ –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö {POSTS_TO_FETCH} –ø–æ—Å—Ç–æ–≤...")
                
                recent_posts = get_user_posts_robust(cl, user_info.pk, POSTS_TO_FETCH)
                print(f"  > –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(recent_posts)}/{POSTS_TO_FETCH} –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")

                calculated_metrics = calculate_metrics(recent_posts, user_info.follower_count)
                
                calculated_metrics_rounded = {
                    'avg_likes': round(calculated_metrics.get('avg_likes', 0)),
                    'avg_comments': round(calculated_metrics.get('avg_comments', 0)),
                    'engagement_rate_percent': round(calculated_metrics.get('engagement_rate_percent', 0.0), 2),
                    'posting_frequency_days': round(calculated_metrics.get('posting_frequency_days', 0.0), 1) if calculated_metrics.get('posting_frequency_days') is not None else None,
                    'activity_score': calculated_metrics.get('activity_score', 0.0)
                }

                lead_analysis_data = calculate_lead_analysis(profile_data, calculated_metrics_rounded)
                print(f"‚≠ê –ê–Ω–∞–ª–∏–∑ –ª–∏–¥–∞: –∏—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª {lead_analysis_data.get('final_score')}, –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç '{lead_analysis_data.get('priority')}'")
                
                result_row.update(calculated_metrics_rounded)
                result_row.update({f"la_{k}": v for k, v in lead_analysis_data.items()})
                
                posts_details = []
                for post in recent_posts:
                    media_urls = [r.thumbnail_url for r in post.resources] if post.media_type == 8 else ([post.thumbnail_url] if post.thumbnail_url else [])
                    posts_details.append({'post_id': post.pk, 'date': post.taken_at.isoformat(), 'caption': (post.caption_text or '')[:CAPTION_TRUNCATE_LIMIT], 'likes': post.like_count, 'comments': post.comment_count, 'media_urls': [str(url) for url in media_urls if url], 'has_location': bool(post.location), 'hashtags': extract_hashtags(post.caption_text), 'is_video': post.media_type == 2})
                
                all_users_full_data.append({
                    "profile_info": profile_data, "calculated_metrics": calculated_metrics_rounded,
                    "lead_analysis": lead_analysis_data, "recent_posts": posts_details
                })
                
                print(f"‚úÖ –ü—Ä–æ—Ñ–∏–ª—å {username} —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω.")

            except UserNotFound:
                result_row['status'] = 'NOT_FOUND'; print(f"‚ùì –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {username} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            except Exception as e:
                result_row['status'] = 'ERROR'; print(f"‚ùå –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {username}: {e}")
            
            writer.writerow(result_row)
            human_delay(8, 20)

    with open(OUTPUT_JSON_REPORT_FILE, 'w', encoding='utf-8') as json_file:
        json.dump(all_users_full_data, json_file, indent=4, ensure_ascii=False)

    print("\nüéâüéâüéâ –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! üéâüéâüéâ")
    print(f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {FOUND_USERS_CSV_FILE}")
    print(f"üìä –°–≤–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUTPUT_CSV_FILE}")
    print(f"üìÑ –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {OUTPUT_JSON_REPORT_FILE}")


if __name__ == "__main__":
    if USERNAME == "your_instagram_username" or PASSWORD == "your_instagram_password":
        print("üî¥ –í–ù–ò–ú–ê–ù–ò–ï: –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–∫–∞–∂–∏—Ç–µ –≤–∞—à–∏ USERNAME –∏ PASSWORD –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö —Å–∫—Ä–∏–ø—Ç–∞!")
    else:
        main()
